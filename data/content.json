{
  "meta": {
    "title": "AI Coding Literacy",
    "subtitle": "Eine Lernplattform für Wissenschaftler·innen zur systematischen Entwicklung von Kompetenzen im Umgang mit KI-gestützter Programmierung.",
    "description": "AI Coding Literacy bezeichnet die Kompetenz, Large Language Models als Werkzeuge zur Codeentwicklung einzusetzen. Das Ziel ist nicht professionelle Softwareentwicklung, sondern Scripting und Prototyping: kleine, funktionale Lösungen für konkrete Probleme aus dem eigenen Arbeitsbereich."
  },

  "audience": {
    "description": "Fachwissenschaftler·innen aus dem geisteswissenschaftlich-kulturwissenschaftlichen Bereich, die ohne Programmiervorerfahrung kleine, funktionale Tools für ihre Arbeit entwickeln wollen.",
    "prerequisites": [
      "Keine Programmiervorerfahrung erforderlich",
      "Grundlegende Computerkenntnisse (Dateisystem, Browser)",
      "Erfahrung mit einem LLM (Chat-Interface)",
      "Konkrete Probleme aus dem eigenen Arbeitsbereich"
    ]
  },

  "chapters": [
    {
      "id": "CT",
      "name": "Computational Thinking",
      "color": "#4A7C7C",
      "short": "Probleme strukturieren und zerlegen",

      "theory": {
        "description": "Computational Thinking ist die Fähigkeit, Probleme so zu strukturieren, dass sie systematisch gelöst werden können. Für die Arbeit mit LLMs ist das die wichtigste Grundkompetenz. Das eigene Domänenwissen wird zum Ausgangspunkt: Probleme so zerlegen, dass jeder Teilschritt ein präziser Prompt werden kann.",
        "keyPoints": [
          "Computational Thinking ist keine Programmierung, sondern eine Denkweise",
          "Die vier Kernelemente: Dekomposition, Mustererkennung, Abstraktion, Algorithmus",
          "LLMs arbeiten am besten mit klaren, strukturierten Aufgaben – vage Anfragen führen zu unvorhersehbaren Ergebnissen",
          "Dekomposition hilft: bessere Prompts schreiben, Fehler lokalisieren, iterativ verbessern",
          "Pseudocode und Flussdiagramme helfen, bevor Code generiert wird"
        ],
        "concepts": [
          {
            "term": "Dekomposition",
            "definition": "Großes Problem in kleinere Teile zerlegen. Beispiel: 'Katalog digitalisieren' → Scannen → OCR → Korrektur → Strukturieren"
          },
          {
            "term": "Mustererkennung",
            "definition": "Wiederkehrende Strukturen identifizieren. Beispiel: 'Alle Einträge haben: Titel, Datum, Material'"
          },
          {
            "term": "Abstraktion",
            "definition": "Unwichtiges weglassen, Kern erfassen. Beispiel: 'Egal ob Gemälde oder Skulptur – beide haben Inventarnummer'"
          },
          {
            "term": "Algorithmus",
            "definition": "Schrittweise Anleitung formulieren. Beispiel: '1. Öffne Datei, 2. Lies Zeile, 3. Extrahiere Titel...'"
          },
          {
            "term": "Pseudocode",
            "definition": "Informelle Beschreibung eines Ablaufs in natürlicher Sprache, bevor echter Code geschrieben wird"
          }
        ]
      },

      "handsOn": [
        {
          "id": "CT-1",
          "title": "Code als Lesestoff",
          "summary": "Code als Text betrachten, der gelesen und verstanden werden kann – ähnlich wie Quellentexte in der wissenschaftlichen Arbeit.",
          "goals": [
            "Die Grundstruktur eines Python-Programms erkennen",
            "Zeilennummern und Einrückungen als Strukturelemente verstehen",
            "Kommentare als Annotationen interpretieren"
          ],
          "exercise": {
            "description": "Betrachten Sie das folgende Python-Programm. Es lädt ein Bild und zeigt dessen Dimensionen an.",
            "code": "# Bildanalyse-Skript\nfrom PIL import Image\nimport os\n\ndef analyse_image(pfad):\n    \"\"\"Analysiert ein Bild und gibt Dimensionen zurück.\"\"\"\n    bild = Image.open(pfad)\n    breite, hoehe = bild.size\n    return breite, hoehe\n\n# Hauptprogramm\nif __name__ == \"__main__\":\n    b, h = analyse_image(\"beispiel.jpg\")\n    print(f\"Größe: {b} x {h} Pixel\")",
            "filename": "bildanalyse.py",
            "task": "Formulieren Sie einen Prompt, der diesen Code erklärt. Nutzen Sie das Muster: 'Erkläre mir den folgenden Python-Code Zeile für Zeile. Ich bin [Ihre Fachrichtung] und habe keine Programmiererfahrung.'"
          },
          "reflection": [
            "Welche Teile des Codes haben Sie auch ohne Erklärung verstanden?",
            "Welche Begriffe waren unbekannt?",
            "Wie hilfreich war die LLM-Erklärung? Was hat gefehlt?"
          ]
        },
        {
          "id": "CT-2",
          "title": "Probleme zerlegen",
          "summary": "Wie man Probleme so strukturiert, dass LLMs sie verstehen und lösen können.",
          "goals": [
            "Probleme in Teilschritte zerlegen (Dekomposition)",
            "Wiederkehrende Muster erkennen",
            "Vom konkreten Problem zur allgemeinen Lösung abstrahieren"
          ],
          "exercise": {
            "description": "Nehmen Sie ein Problem aus Ihrem Arbeitsbereich und zerlegen Sie es in Teilschritte.",
            "task": "Schreiben Sie Pseudocode, bevor Sie einen Prompt formulieren. Das zwingt Sie, das Problem wirklich zu verstehen."
          },
          "reflection": [
            "Hat das Zerlegen geholfen, das Problem klarer zu sehen?",
            "Wo waren die Grenzen zwischen den Teilschritten unklar?",
            "Was haben Sie über Ihr eigenes Problemverständnis gelernt?"
          ]
        },
        {
          "id": "CT-3",
          "title": "Workflow-Automatisierung",
          "summary": "Wiederkehrende Aufgaben automatisieren. Vom Einzelskript zum wiederverwendbaren Workflow.",
          "goals": [
            "Automatisierungspotenzial erkennen",
            "Batch-Verarbeitung verstehen",
            "Einfache Workflow-Skripte lesen"
          ],
          "exercise": {
            "description": "Verstehen Sie diesen Batch-Verarbeitungs-Workflow.",
            "code": "from pathlib import Path\nimport pandas as pd\n\n# Alle CSV-Dateien im Ordner finden\neingabe_ordner = Path(\"rohdaten\")\nausgabe_ordner = Path(\"verarbeitet\")\nausgabe_ordner.mkdir(exist_ok=True)\n\nfor datei in eingabe_ordner.glob(\"*.csv\"):\n    print(f\"Verarbeite: {datei.name}\")\n    \n    # Laden und bereinigen\n    df = pd.read_csv(datei)\n    df = df.dropna()\n    df.columns = [c.lower().strip() for c in df.columns]\n    \n    # Speichern\n    ausgabe = ausgabe_ordner / datei.name\n    df.to_csv(ausgabe, index=False)\n    \nprint(f\"Fertig! {len(list(eingabe_ordner.glob('*.csv')))} Dateien verarbeitet.\")",
            "filename": "batch_workflow.py",
            "task": "Was macht das Skript? Welche Schritte werden für jede Datei ausgeführt?"
          },
          "reflection": [
            "Wo in Ihrer Arbeit gibt es wiederkehrende Aufgaben?",
            "Was müsste angepasst werden für Ihre Dateien?",
            "Wie würden Sie den Workflow erweitern?"
          ]
        }
      ],

      "resources": [
        {
          "title": "Wing, J. (2006). Computational Thinking",
          "url": "https://www.cs.cmu.edu/~15110-s13/Wing06-ct.pdf",
          "type": "paper"
        },
        {
          "title": "Denning, P. & Tedre, M. (2019). Computational Thinking",
          "url": "https://mitpress.mit.edu/books/computational-thinking",
          "type": "book"
        }
      ],

      "quote": {
        "text": "Computational thinking is a way of solving problems, designing systems, and understanding human behavior that draws on concepts fundamental to computer science.",
        "source": "Jeannette Wing, 2006"
      }
    },

    {
      "id": "RE",
      "name": "Requirement Engineering",
      "color": "#8B4557",
      "short": "Anforderungen präzise formulieren",

      "theory": {
        "description": "Requirement Engineering bezeichnet die systematische Formulierung von Anforderungen. Für die Arbeit mit LLMs ist das entscheidend, weil ein Prompt funktional ein Requirements-Dokument ist. Wer präzise Anforderungen formulieren kann, schreibt bessere Prompts. Die Kernfragen: Was soll das Tool tun? Was nicht? Welche Eingaben, welche Ausgaben? Woran erkennt man, dass es korrekt funktioniert?",
        "keyPoints": [
          "Ein Prompt ist funktional ein Requirements-Dokument",
          "User Stories sind niedrigschwelliger als formale Spezifikationen",
          "LLMs raten bei vagen Anforderungen – sie füllen Lücken mit Annahmen",
          "Akzeptanzkriterien definieren, bevor Code generiert wird",
          "Grenzfälle und Ausnahmen antizipieren",
          "Das LLM kann nur liefern, was Sie spezifizieren"
        ],
        "concepts": [
          {
            "term": "User Story",
            "definition": "Leichtgewichtiges Anforderungsformat: 'Als [Rolle] möchte ich [Funktion], damit [Nutzen]'"
          },
          {
            "term": "INVEST-Kriterien",
            "definition": "Gute Anforderungen sind: Independent, Negotiable, Valuable, Estimable, Small, Testable"
          },
          {
            "term": "Kernfragen",
            "definition": "Was tun? Was nicht? Eingabe? Ausgabe? Woran erkenne ich Erfolg?"
          },
          {
            "term": "Funktionale Anforderung",
            "definition": "Was das System tun soll (Eingabe → Verarbeitung → Ausgabe)"
          },
          {
            "term": "Akzeptanzkriterium",
            "definition": "Messbarer Maßstab, ob eine Anforderung erfüllt ist"
          },
          {
            "term": "Grenzfall",
            "definition": "Extreme oder ungewöhnliche Eingaben, die getestet werden müssen"
          }
        ]
      },

      "handsOn": [
        {
          "id": "RE-1",
          "title": "User Stories schreiben",
          "summary": "Anforderungen im User-Story-Format formulieren, bevor Sie prompten.",
          "goals": [
            "Das User-Story-Format anwenden",
            "Akzeptanzkriterien definieren",
            "Abgrenzungen explizit machen"
          ],
          "exercise": {
            "description": "Sie möchten ein Tool, das Ihre Bibliografie verwaltet. Formulieren Sie die Anforderungen.",
            "code": "# Vage Anforderung (schlecht):\n\"Ich brauche was, um meine Bibliografie zu verwalten\"\n\n# User Story (besser):\n\"Als Forschende möchte ich eine CSV-Datei mit\nBibliografiedaten in eine formatierte HTML-Seite\numwandeln, damit ich meine Publikationsliste\nauf meiner Website anzeigen kann.\"\n\n# Präzise Spezifikation (am besten):\n# Eingabe: CSV mit Spalten (Autor, Titel, Jahr, Zeitschrift)\n# Ausgabe: HTML mit alphabetisch sortierter Liste\n# Format: Autor (Jahr): Titel. In: Zeitschrift.\n# Nicht im Scope: PDF-Verlinkung, Dubletten-Erkennung",
            "filename": "anforderung.txt",
            "task": "Schreiben Sie für Ihr eigenes Projekt: 1) Eine User Story, 2) Eingabe und Ausgabe, 3) Drei Akzeptanzkriterien, 4) Zwei Abgrenzungen (Was soll es nicht tun?)"
          },
          "reflection": [
            "Welche Anforderungen waren implizit (in Ihrem Kopf)?",
            "Was hätte das LLM ohne diese Spezifikation geraten?",
            "Wie schwer war es, Abgrenzungen zu formulieren?"
          ]
        },
        {
          "id": "RE-2",
          "title": "Akzeptanzkriterien testen",
          "summary": "Code systematisch gegen definierte Kriterien prüfen.",
          "goals": [
            "Testfälle aus Anforderungen ableiten",
            "Systematisch Grenzfälle durchspielen",
            "Abweichungen dokumentieren"
          ],
          "exercise": {
            "description": "Nehmen Sie den generierten Code aus RE-1 und testen Sie ihn gegen Ihre Akzeptanzkriterien.",
            "task": "Erstellen Sie eine Checkliste und prüfen Sie jeden Punkt. Dokumentieren Sie, was funktioniert und was nicht."
          },
          "reflection": [
            "Wie viele Ihrer Kriterien wurden erfüllt?",
            "Welche Fehler hätten Sie ohne Kriterien übersehen?",
            "Was müssen Sie im Prompt anpassen?"
          ]
        }
      ],

      "resources": [
        {
          "title": "Vogelsang & Fischbach (2024): LLMs for RE Tasks",
          "url": "https://arxiv.org/abs/2402.13823",
          "type": "paper"
        },
        {
          "title": "Lucassen et al. (2016): User Stories in Practice",
          "url": "https://link.springer.com/chapter/10.1007/978-3-319-30282-9_14",
          "type": "paper"
        },
        {
          "title": "User Stories Applied (Mike Cohn)",
          "url": "https://www.mountaingoatsoftware.com/books/user-stories-applied",
          "type": "book"
        }
      ],

      "quote": {
        "text": "The hardest single part of building a software system is deciding precisely what to build.",
        "source": "Fred Brooks, No Silver Bullet"
      }
    },

    {
      "id": "CE",
      "name": "Context Engineering",
      "color": "#5B7355",
      "short": "Kontext für LLMs gestalten",

      "theory": {
        "description": "Context Engineering bezeichnet die systematische Gestaltung des Informationskontexts, den man einem LLM gibt. Das Kontextfenster ist begrenzt – was man hineinpackt und wie man es anordnet, bestimmt die Qualität der Ausgabe. Mehr ist nicht automatisch besser: LLMs können Informationen 'in der Mitte verlieren' (Lost in the Middle Problem).",
        "keyPoints": [
          "Kontextfenster: GPT-4o 128K, Claude 200K, Gemini 1M Tokens",
          "Mehr Kontext ist nicht automatisch besser – Lost in the Middle Problem",
          "Bewährte Struktur: Rolle → Aufgabe → Infos → Constraints → Format",
          "Komprimierung: Relevante Ausschnitte statt ganzer Dateien",
          "Beispiele sind oft wirksamer als lange Beschreibungen",
          "Projektkonventionen explizit machen"
        ],
        "concepts": [
          {
            "term": "RAG",
            "definition": "Retrieval-Augmented Generation – relevante Infos aus externen Quellen abrufen und dem LLM bereitstellen"
          },
          {
            "term": "Lost in the Middle",
            "definition": "LLMs verlieren Informationen in der Mitte langer Kontexte – wichtiges an Anfang/Ende"
          },
          {
            "term": "Kontextstruktur",
            "definition": "Bewährte Anordnung: Rolle → Aufgabe → Informationen → Constraints → Format"
          },
          {
            "term": "Kontextfenster",
            "definition": "Die maximale Textmenge (in Tokens), die ein LLM gleichzeitig verarbeiten kann"
          },
          {
            "term": "Destillation",
            "definition": "Komplexe Informationen auf das Wesentliche komprimieren"
          }
        ]
      },

      "handsOn": [
        {
          "id": "CE-1",
          "title": "Kontext strukturieren",
          "summary": "Einen Prompt mit der bewährten Kontextstruktur aufbauen.",
          "goals": [
            "Die 5-teilige Kontextstruktur anwenden",
            "Relevante von irrelevanten Informationen trennen",
            "Wirksamkeit vergleichen"
          ],
          "exercise": {
            "description": "Vergleichen Sie diese beiden Prompts für dieselbe Aufgabe.",
            "code": "# OHNE Kontextstruktur:\n\"Mach mir ein Skript für meine Excel-Dateien\"\n\n# MIT Kontextstruktur:\n\"\"\"\nDu hilfst mir, ein Python-Skript zu schreiben.\n\nAufgabe: Alle Excel-Dateien in einem Ordner einlesen\nund zu einer CSV zusammenführen.\n\nBeispieldaten (erste Datei):\n| Inv-Nr | Bezeichnung | Datierung |\n| 001    | Vase        | 1820      |\n\nConstraints:\n- Alle Dateien haben identische Spaltenstruktur\n- Encoding: UTF-8\n- Nur .xlsx, keine .xls\n\nAusgabe: Ein einzelnes Skript mit Kommentaren.\n\"\"\"",
            "filename": "kontext_vergleich.txt",
            "task": "Testen Sie beide Prompts mit einem LLM. Dokumentieren Sie die Unterschiede in den Ergebnissen."
          },
          "reflection": [
            "Welche Rückfragen stellte das LLM beim ersten Prompt?",
            "Wie viel präziser war das Ergebnis beim zweiten Prompt?",
            "Welche Informationen haben Sie im strukturierten Prompt hinzugefügt?"
          ]
        },
        {
          "id": "CE-2",
          "title": "Wissen destillieren",
          "summary": "Einen komplexen Standard so aufbereiten, dass er ins Kontextfenster passt.",
          "goals": [
            "Relevante Informationen identifizieren",
            "Kontext strukturiert komprimieren",
            "Wirksamkeit testen"
          ],
          "exercise": {
            "description": "Besuchen Sie die Darwin-Core-Dokumentation (dwc.tdwg.org) und destillieren Sie die relevanten Felder für eine Museumssammlung.",
            "task": "Erstellen Sie ein kompaktes Referenzdokument mit Feldname, Definition und Beispielwert. Testen Sie: Kann das LLM damit korrekt arbeiten?"
          },
          "reflection": [
            "Wie viel kürzer wurde das Dokument?",
            "Welche Informationen haben Sie weggelassen?",
            "Hat das LLM mit dem destillierten Kontext korrekt gearbeitet?"
          ]
        }
      ],

      "resources": [
        {
          "title": "Gao et al. (2024): RAG Survey",
          "url": "https://arxiv.org/abs/2312.10997",
          "type": "paper"
        },
        {
          "title": "Liu et al. (2025): Long-Context Survey",
          "url": "https://arxiv.org/abs/2503.17407",
          "type": "paper"
        },
        {
          "title": "Anthropic: Prompt Engineering Guide",
          "url": "https://docs.anthropic.com/claude/docs/prompt-engineering",
          "type": "documentation"
        }
      ],

      "quote": {
        "text": "Context is everything. The same prompt with different context produces completely different results.",
        "source": "Anthropic Documentation"
      }
    },

    {
      "id": "PE",
      "name": "Prompt Engineering",
      "color": "#7B6B8D",
      "short": "Effektive Prompts entwickeln",

      "theory": {
        "description": "Prompt Engineering bezeichnet die Entwicklung und Optimierung von Eingabeaufforderungen. Ein Prompt ist mehr als eine Frage – die Art, wie man fragt, bestimmt die Art der Antwort. Schulhoff et al. (2024) dokumentieren 58 verschiedene Prompting-Techniken. Für die meisten Anwendungen genügen wenige Kernstrategien.",
        "keyPoints": [
          "Schulhoff et al. dokumentieren 58 Prompting-Techniken – für die Praxis genügen wenige",
          "Zero-Shot: Direkte Anweisung ohne Beispiele – für einfache, eindeutige Aufgaben",
          "Few-Shot: 2-5 Beispiele zeigen Format und Umgang mit Sonderfällen",
          "Chain-of-Thought: 'Denke Schritt für Schritt' – für komplexe Probleme",
          "Iterative Verfeinerung: Prompt → Output → Problem → Anpassung → wiederholen",
          "Ausgabeformat explizit spezifizieren"
        ],
        "concepts": [
          {
            "term": "Zero-Shot",
            "definition": "Direkte Anweisung ohne Beispiele – funktioniert bei einfachen, eindeutigen Aufgaben"
          },
          {
            "term": "Few-Shot",
            "definition": "2-5 Beispiele im Prompt zeigen das gewünschte Format und den Umgang mit Sonderfällen"
          },
          {
            "term": "Chain-of-Thought",
            "definition": "'Denke Schritt für Schritt' – verbessert Leistung bei komplexen, mehrstufigen Problemen"
          },
          {
            "term": "Role Prompting",
            "definition": "'Du bist ein erfahrener...' – wenn spezifische Expertise oder Perspektive wichtig ist"
          },
          {
            "term": "Iteration",
            "definition": "Prompt → Output prüfen → Problem identifizieren → Prompt anpassen → wiederholen"
          }
        ]
      },

      "handsOn": [
        {
          "id": "PE-1",
          "title": "Zero-Shot vs. Few-Shot",
          "summary": "Den Unterschied zwischen direkter Anweisung und Beispiel-basiertem Prompting verstehen.",
          "goals": [
            "Zero-Shot und Few-Shot unterscheiden",
            "Erkennen, wann Beispiele helfen",
            "Beispiele effektiv formulieren"
          ],
          "exercise": {
            "description": "Vergleichen Sie diese beiden Strategien für dieselbe Aufgabe.",
            "code": "# ZERO-SHOT:\n\"Extrahiere das Datum aus diesem Text:\n'Das Gemälde wurde am 15. März 1823 erworben.'\"\n\n# FEW-SHOT:\n\"Extrahiere das Datum aus Texten. Formatiere als YYYY-MM-DD.\n\nText: 'Erworben im Januar 1820' -> 1820-01-00\nText: 'Datiert auf den 3.5.1899' -> 1899-05-03\nText: 'Das Gemälde wurde am 15. März 1823 erworben.' ->\"",
            "filename": "prompting_strategien.txt",
            "task": "Testen Sie beide Strategien. Wann liefert Few-Shot bessere Ergebnisse? Wann genügt Zero-Shot?"
          },
          "reflection": [
            "Wie unterschieden sich die Ausgaben?",
            "Welche Sonderfälle haben die Beispiele abgedeckt?",
            "Wann würden Sie Few-Shot in Ihrer Arbeit einsetzen?"
          ]
        },
        {
          "id": "PE-2",
          "title": "Iteratives Prompting",
          "summary": "Durch Dialog schrittweise zum gewünschten Ergebnis.",
          "goals": [
            "Feedback präzise formulieren",
            "Typische Anpassungen kennen",
            "Wissen, wann man neu anfangen sollte"
          ],
          "exercise": {
            "description": "Verfolgen Sie diesen iterativen Verfeinerungsprozess.",
            "code": "# Iteration 1:\n\"Schreibe ein Skript, das Bilder verkleinert.\"\n# Problem: Unklar welche Größe, welches Format, welcher Ordner\n\n# Iteration 2:\n\"Schreibe ein Python-Skript mit Pillow.\nEingabe: Ordnerpfad als Argument\nAufgabe: Alle .jpg auf max. 1200px Breite skalieren\nAusgabe: Neue Dateien mit Suffix '_web'\"\n# Problem: Überschreibt Originale\n\n# Iteration 3:\n\"... (wie oben)\nConstraint: Originaldateien nicht überschreiben\"",
            "filename": "iteration.txt",
            "task": "Starten Sie mit einem einfachen Prompt für Ihr Projekt. Dokumentieren Sie 3 Iterationen: Problem → Anpassung → Ergebnis."
          },
          "reflection": [
            "Wie viele Iterationen brauchten Sie?",
            "Welche Art von Feedback war am effektivsten?",
            "Wann hätten Sie besser neu angefangen?"
          ]
        }
      ],

      "resources": [
        {
          "title": "Schulhoff et al. (2024): The Prompt Report (58 Techniken)",
          "url": "https://arxiv.org/abs/2406.06608",
          "type": "paper"
        },
        {
          "title": "Wei et al. (2022): Chain-of-Thought (NeurIPS)",
          "url": "https://arxiv.org/abs/2201.11903",
          "type": "paper"
        },
        {
          "title": "OpenAI: Prompt Engineering Guide",
          "url": "https://platform.openai.com/docs/guides/prompt-engineering",
          "type": "documentation"
        }
      ],

      "quote": {
        "text": "The quality of your output is directly proportional to the quality of your input.",
        "source": "Prompt Engineering Wisdom"
      }
    },

    {
      "id": "CL",
      "name": "Code Literacy",
      "color": "#8B7355",
      "short": "Generierten Code verstehen",

      "theory": {
        "description": "Code Literacy bezeichnet die Fähigkeit, Code zu lesen und zu verstehen. Das ist eine eigenständige Kompetenz, die dem Schreiben vorausgeht. Man muss nicht selbst Code schreiben können, um LLM-generierten Code zu bewerten. Die Skill-Hierarchie: Run → Trace → Explain → Write. Für AI Coding Literacy sind primär Trace und Explain relevant.",
        "keyPoints": [
          "Lesen und Schreiben sind getrennte Kompetenzen (Lopez et al., 2008)",
          "Skill-Hierarchie: Run → Trace → Explain → Write",
          "Auch Experten vermeiden tiefes Verstehen wenn möglich – partielles Verstehen ist normal",
          "39% der Praktiker finden LLM-Code lesbarer als menschlichen Code",
          "Zwei mentale Modelle: prozedural ('Was passiert der Reihe nach?') und funktional ('Was ist das Ziel?')",
          "Sie müssen nicht alles verstehen – nur genug, um zu prüfen"
        ],
        "concepts": [
          {
            "term": "Skill-Hierarchie",
            "definition": "Run → Trace → Explain → Write: Lesen und Nachvollziehen gehen dem Schreiben voraus"
          },
          {
            "term": "Prozedurales Modell",
            "definition": "'Was passiert der Reihe nach?' – Kontrollfluss, Zeile für Zeile nachvollziehen"
          },
          {
            "term": "Funktionales Modell",
            "definition": "'Was ist das Ziel?' – Zweck verstehen, Eingabe/Ausgabe identifizieren"
          },
          {
            "term": "Delocalized Plans",
            "definition": "Code einer Intention ist oft über mehrere Stellen verstreut – Navigation erforderlich"
          },
          {
            "term": "Kontrollstruktur",
            "definition": "Anweisungen, die den Ablauf steuern (if, for, while)"
          },
          {
            "term": "Bibliothek",
            "definition": "Sammlung von vorgefertigtem Code für bestimmte Aufgaben"
          }
        ]
      },

      "handsOn": [
        {
          "id": "CL-1",
          "title": "Variablen und Datentypen",
          "summary": "Verstehen, wie Daten gespeichert und verarbeitet werden.",
          "goals": [
            "Variablen als benannte Container verstehen",
            "Grundlegende Datentypen unterscheiden",
            "Datenfluss im Code nachvollziehen"
          ],
          "exercise": {
            "description": "Analysieren Sie diesen Code-Ausschnitt.",
            "code": "# Sammlungsobjekt\nobjekt = {\n    \"inventar_nr\": \"2024-001\",\n    \"titel\": \"Bronzefibel\",\n    \"datierung\": -500,\n    \"materialien\": [\"Bronze\", \"Eisen\"]\n}\n\nprint(f\"Objekt {objekt['inventar_nr']}: {objekt['titel']}\")",
            "filename": "sammlung.py",
            "task": "Welche Variablen gibt es? Welche Datentypen haben sie? Was passiert in der letzten Zeile?"
          },
          "reflection": [
            "Konnten Sie den Datenfluss nachvollziehen?",
            "Was würde passieren, wenn 'datierung' ein String wäre?",
            "Welche Teile waren intuitiv verständlich?"
          ]
        },
        {
          "id": "CL-2",
          "title": "Kontrollstrukturen lesen",
          "summary": "Code verzweigt und wiederholt sich. Kontrollstrukturen steuern den Fluss.",
          "goals": [
            "If-else-Verzweigungen verstehen",
            "Schleifen nachvollziehen",
            "Verschachtelte Strukturen entwirren"
          ],
          "exercise": {
            "description": "Verfolgen Sie den Ablauf dieses Codes.",
            "code": "objekte = [\n    {\"titel\": \"Vase\", \"zustand\": \"gut\"},\n    {\"titel\": \"Münze\", \"zustand\": \"beschädigt\"},\n    {\"titel\": \"Ring\", \"zustand\": \"gut\"}\n]\n\nfuer_ausstellung = []\nfuer_restaurierung = []\n\nfor obj in objekte:\n    if obj[\"zustand\"] == \"gut\":\n        fuer_ausstellung.append(obj[\"titel\"])\n    else:\n        fuer_restaurierung.append(obj[\"titel\"])\n\nprint(f\"Ausstellung: {fuer_ausstellung}\")\nprint(f\"Restaurierung: {fuer_restaurierung}\")",
            "filename": "sortierung.py",
            "task": "Was steht am Ende in 'fuer_ausstellung' und 'fuer_restaurierung'? Zeichnen Sie den Ablauf auf Papier."
          },
          "reflection": [
            "Konnten Sie vorhersagen, was der Code ausgibt?",
            "Wo hat das Aufzeichnen geholfen?",
            "Was wäre anders bei einer anderen Bedingung?"
          ]
        },
        {
          "id": "CL-3",
          "title": "Funktionen und Bibliotheken",
          "summary": "Wiederverwendbare Bausteine und externe Pakete verstehen.",
          "goals": [
            "Funktionsdefinitionen erkennen",
            "Parameter und Rückgabewerte identifizieren",
            "Import-Statements verstehen"
          ],
          "exercise": {
            "description": "Analysieren Sie diese Funktion zur Textbereinigung.",
            "code": "def bereinige_text(text, lowercase=True):\n    \"\"\"Bereinigt einen Text für die Analyse.\"\"\"\n    text = text.strip()\n    \n    if lowercase:\n        text = text.lower()\n    \n    import re\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text\n\n# Verwendung\noriginal = \"  Hallo    WELT  \"\nbereinigt = bereinige_text(original)\nprint(bereinigt)  # \"hallo welt\"",
            "filename": "textbereinigung.py",
            "task": "Was sind die Parameter? Was ist der Rückgabewert? Was macht die Bibliothek 're'?"
          },
          "reflection": [
            "Könnten Sie die Funktion mit anderen Parametern aufrufen?",
            "Wo würde diese Funktion in Ihrer Arbeit nützlich sein?",
            "Welche Teile waren schwer zu verstehen?"
          ]
        },
        {
          "id": "CL-4",
          "title": "Pandas für Datenverarbeitung",
          "summary": "DataFrames sind wie Excel-Tabellen, aber mächtiger.",
          "goals": [
            "DataFrames als Datenstruktur verstehen",
            "Filtern, Sortieren, Gruppieren lesen",
            "Daten einlesen und exportieren"
          ],
          "exercise": {
            "description": "Verstehen Sie diese Datenanalyse.",
            "code": "import pandas as pd\n\ndf = pd.read_csv(\"sammlung.csv\")\n\n# Nur Objekte aus Bronze\nbronze = df[df[\"material\"] == \"Bronze\"]\n\n# Nach Datierung sortieren\nbronze_sortiert = bronze.sort_values(\"datierung\")\n\nprint(f\"Anzahl Bronze-Objekte: {len(bronze)}\")\nprint(f\"Ältestes: {bronze_sortiert.iloc[0]['titel']}\")",
            "filename": "pandas_analyse.py",
            "task": "Was passiert in jeder Zeile? Was ist df, bronze, bronze_sortiert?"
          },
          "reflection": [
            "Was bedeutet die eckige Klammer-Notation?",
            "Wie würden Sie nach einem anderen Material filtern?",
            "Welche Operationen könnten Sie für Ihre Daten brauchen?"
          ]
        },
        {
          "id": "CL-5",
          "title": "Visualisierung mit Matplotlib",
          "summary": "Daten sichtbar machen.",
          "goals": [
            "Grundstruktur von Matplotlib-Code verstehen",
            "Figure und Axes unterscheiden",
            "Anpassungen lesen"
          ],
          "exercise": {
            "description": "Verstehen Sie diese Visualisierung.",
            "code": "import matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.read_csv(\"objekte_pro_jahr.csv\")\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.bar(df[\"jahr\"], df[\"anzahl\"], color=\"#4A7C7C\")\nax.set_xlabel(\"Jahr\")\nax.set_ylabel(\"Anzahl Objekte\")\nax.set_title(\"Sammlungszuwachs über Zeit\")\n\nplt.tight_layout()\nplt.savefig(\"zuwachs.png\", dpi=150)",
            "filename": "visualisierung.py",
            "task": "Was wird visualisiert? Wie würden Sie die Farbe ändern?"
          },
          "reflection": [
            "Konnten Sie sich das Ergebnis vorstellen?",
            "Was wäre anders bei einem Liniendiagramm?",
            "Welche Visualisierungen brauchen Sie für Ihre Daten?"
          ]
        }
      ],

      "resources": [
        {
          "title": "Lopez et al. (2008): Reading vs Writing Skills",
          "url": "https://dl.acm.org/doi/10.1145/1404520.1404535",
          "type": "paper"
        },
        {
          "title": "Xie et al. (2019): Quadranten-Modell",
          "url": "https://www.tandfonline.com/doi/full/10.1080/08993408.2019.1565235",
          "type": "paper"
        },
        {
          "title": "Hermans, F. (2021). The Programmer's Brain",
          "url": "https://www.manning.com/books/the-programmers-brain",
          "type": "book"
        },
        {
          "title": "Python Tutorial (offiziell)",
          "url": "https://docs.python.org/3/tutorial/",
          "type": "documentation"
        }
      ],

      "quote": {
        "text": "Reading code is a skill. Like reading a foreign language, it gets easier with practice.",
        "source": "Felienne Hermans"
      }
    },

    {
      "id": "RV",
      "name": "Review",
      "color": "#4A6B8C",
      "short": "Ergebnisse systematisch prüfen",

      "theory": {
        "description": "Review bezeichnet die systematische Prüfung von generiertem Code gegen die definierten Anforderungen. LLM-generierter Code sieht oft korrekt aus, enthält aber systematisch Fehler. EvalPlus zeigt: Pass-Raten sinken um 19-28.9% bei gründlicherem Testing. Tambon et al. identifizierten 10 Bug-Patterns. Vertrauen ist gut, Prüfen ist besser.",
        "keyPoints": [
          "EvalPlus: Pass-Raten sinken um 19-28.9% bei gründlicherem Testing",
          "Tambon et al.: 10 Bug-Patterns in LLM-generiertem Code",
          "29.5% Python-Snippets von Copilot haben Sicherheitslücken (Fu et al.)",
          "5 Halluzinationskategorien: Intent Conflicting, Context Inconsistency, Dead Code, Knowledge Conflicting",
          "Metamorphic Testing: Paraphrasierte Prompts zur Konsistenzprüfung",
          "Systematisch testen: Normalfall, Grenzfall, Fehlerfall"
        ],
        "concepts": [
          {
            "term": "Halluzinierte Objekte",
            "definition": "Nicht existierende Funktionen oder Bibliotheken, die das LLM erfindet"
          },
          {
            "term": "Intent Conflicting",
            "definition": "Code widerspricht der Anforderung – macht etwas anderes als gewünscht"
          },
          {
            "term": "Context Inconsistency",
            "definition": "Code widerspricht dem bereitgestellten Kontext (Beispieldaten, Dokumentation)"
          },
          {
            "term": "Metamorphic Testing",
            "definition": "Paraphrasierte Prompts zur Konsistenzprüfung – erkennt 75% fehlerhafter Programme"
          },
          {
            "term": "Traceback",
            "definition": "Die Fehlerspur, die zeigt, wo der Fehler aufgetreten ist – von unten nach oben lesen"
          },
          {
            "term": "Logischer Fehler",
            "definition": "Code läuft, aber das Ergebnis ist falsch – am schwierigsten zu finden"
          }
        ]
      },

      "handsOn": [
        {
          "id": "RV-1",
          "title": "Fehlermeldungen lesen",
          "summary": "Fehlermeldungen sind Informationen. Lesen lernen ist eine Kernkompetenz.",
          "goals": [
            "Fehlermeldungen strukturiert lesen",
            "Häufige Fehlertypen unterscheiden",
            "Systematisch nach Ursachen suchen"
          ],
          "exercise": {
            "description": "Lesen Sie diese Fehlermeldung und finden Sie die Ursache.",
            "code": "Traceback (most recent call last):\n  File \"analyse.py\", line 12, in <module>\n    ergebnis = berechne_durchschnitt(werte)\n  File \"analyse.py\", line 5, in berechne_durchschnitt\n    return sum(zahlen) / len(zahlen)\nZeroDivisionError: division by zero",
            "filename": "fehler.txt",
            "task": "Was ist passiert? In welcher Zeile? Warum? Wie würden Sie das Problem beheben?"
          },
          "reflection": [
            "Konnten Sie den Traceback von unten nach oben lesen?",
            "War die Fehlerursache klar?",
            "Wie würden Sie diesen Fehler im Dialog mit dem LLM beheben?"
          ]
        },
        {
          "id": "RV-2",
          "title": "Halluzinationen erkennen",
          "summary": "LLMs erfinden manchmal plausibel klingende aber falsche Informationen.",
          "goals": [
            "Halluzinationsrisiken erkennen",
            "Kritische Bereiche identifizieren",
            "Strategien zur Absicherung entwickeln"
          ],
          "exercise": {
            "description": "Historische Abkürzungen sind anfällig für Halluzinationen.",
            "code": "# Historische Fundortangaben mit Abkürzungen:\n# B Jenesien: auf Porphyr.\n# O Stanzerthal in Spuren.\n# T Innsbruck: am Patscherkofel.\n\n# Prompten Sie ein LLM ohne Kontext:\n# \"Was bedeuten die Abkürzungen B, O, T \n#  in diesen historischen Fundortangaben?\"",
            "task": "Vergleichen Sie die LLM-Antwort mit einer verlässlichen Quelle. Was wurde halluziniert?"
          },
          "reflection": [
            "Wie selbstsicher präsentierte das LLM falsche Informationen?",
            "Welche Strategie hätte das verhindert?",
            "Bei welchen Daten in Ihrer Arbeit besteht Halluzinationsrisiko?"
          ]
        },
        {
          "id": "RV-3",
          "title": "Systematisches Review",
          "summary": "Code gegen Anforderungen prüfen – mit Checkliste.",
          "goals": [
            "Review-Checkliste anwenden",
            "Grenzfälle systematisch testen",
            "Verbesserungen gezielt anfordern"
          ],
          "exercise": {
            "description": "Prüfen Sie generierten Code gegen diese Checkliste.",
            "task": "1) Erfüllt der Code alle Anforderungen? 2) Sind Grenzfälle behandelt? 3) Sind Fehlermeldungen hilfreich? 4) Ist der Code lesbar? 5) Gibt es Sicherheitsrisiken?"
          },
          "reflection": [
            "Wie viele Punkte waren erfüllt?",
            "Welche Probleme hätten Sie ohne Checkliste übersehen?",
            "Wie würden Sie die Checkliste für Ihre Projekte anpassen?"
          ]
        }
      ],

      "resources": [
        {
          "title": "Liu et al. (2024): HalluCode Benchmark",
          "url": "https://arxiv.org/abs/2404.00971",
          "type": "paper"
        },
        {
          "title": "Tambon et al. (2024): Bug-Pattern Taxonomie",
          "url": "https://arxiv.org/abs/2403.08937",
          "type": "paper"
        },
        {
          "title": "Fu et al. (2024): Security Analysis of Copilot",
          "url": "https://dl.acm.org/doi/10.1145/3643769",
          "type": "paper"
        },
        {
          "title": "Mollick, E. (2024). Co-Intelligence",
          "url": "https://www.oneusefulthing.org/",
          "type": "book"
        }
      ],

      "quote": {
        "text": "Trust, but verify. Especially with AI-generated code.",
        "source": "Adapted from Reagan"
      }
    }
  ]
}
